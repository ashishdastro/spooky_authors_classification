{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Implement a Transformer block as a Keras layer and use it for text classification.","metadata":{}},{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-08-31T14:55:29.435782Z","iopub.execute_input":"2022-08-31T14:55:29.436475Z","iopub.status.idle":"2022-08-31T14:55:29.477994Z","shell.execute_reply.started":"2022-08-31T14:55:29.436378Z","shell.execute_reply":"2022-08-31T14:55:29.477057Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"import tensorflow as tf\nfrom tensorflow import keras\nfrom tensorflow.keras import layers, callbacks\nfrom tensorflow.keras.utils import to_categorical\nfrom keras.preprocessing import text, sequence\nfrom sklearn.model_selection import train_test_split\nfrom tensorflow.keras.optimizers import Adam\n\nimport matplotlib.pyplot as plt\nimport pandas as pd\nimport numpy as np\nimport time\n\n#ignore the warnings\nimport warnings\nwarnings.filterwarnings('ignore')\n\n\"\"\"\nImplement multi head self attention as a Keras layer\n\"\"\"\n\nprint(\"Tensorflow Version: \", tf.__version__)\nprint(\"Eager mode enabled: \", tf.executing_eagerly())\nprint(\"GPU available: \", tf.test.is_gpu_available())\n\nnotebookstart = time.time()","metadata":{"execution":{"iopub.status.busy":"2022-08-31T14:55:32.241932Z","iopub.execute_input":"2022-08-31T14:55:32.242398Z","iopub.status.idle":"2022-08-31T14:55:40.647468Z","shell.execute_reply.started":"2022-08-31T14:55:32.242359Z","shell.execute_reply":"2022-08-31T14:55:40.646577Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"class MultiHeadSelfAttention(layers.Layer):\n    def __init__(self, embed_dim, num_heads=8):\n        super(MultiHeadSelfAttention, self).__init__()\n        self.embed_dim = embed_dim\n        self.num_heads = num_heads\n        if embed_dim % num_heads != 0:\n            raise ValueError(\n                f\"embedding dimension = {embed_dim} should be divisible by number of heads = {num_heads}\"\n            )\n        self.projection_dim = embed_dim // num_heads\n        self.query_dense = layers.Dense(embed_dim)\n        self.key_dense = layers.Dense(embed_dim)\n        self.value_dense = layers.Dense(embed_dim)\n        self.combine_heads = layers.Dense(embed_dim)\n\n    def attention(self, query, key, value):\n        score = tf.matmul(query, key, transpose_b=True)\n        dim_key = tf.cast(tf.shape(key)[-1], tf.float32)\n        scaled_score = score / tf.math.sqrt(dim_key)\n        weights = tf.nn.softmax(scaled_score, axis=-1)\n        output = tf.matmul(weights, value)\n        return output, weights\n\n    def separate_heads(self, x, batch_size):\n        x = tf.reshape(x, (batch_size, -1, self.num_heads, self.projection_dim))\n        return tf.transpose(x, perm=[0, 2, 1, 3])\n\n    def call(self, inputs):\n        # x.shape = [batch_size, seq_len, embedding_dim]\n        batch_size = tf.shape(inputs)[0]\n        query = self.query_dense(inputs)  # (batch_size, seq_len, embed_dim)\n        key = self.key_dense(inputs)  # (batch_size, seq_len, embed_dim)\n        value = self.value_dense(inputs)  # (batch_size, seq_len, embed_dim)\n        query = self.separate_heads(\n            query, batch_size\n        )  # (batch_size, num_heads, seq_len, projection_dim)\n        key = self.separate_heads(\n            key, batch_size\n        )  # (batch_size, num_heads, seq_len, projection_dim)\n        value = self.separate_heads(\n            value, batch_size\n        )  # (batch_size, num_heads, seq_len, projection_dim)\n        attention, weights = self.attention(query, key, value)\n        attention = tf.transpose(\n            attention, perm=[0, 2, 1, 3]\n        )  # (batch_size, seq_len, num_heads, projection_dim)\n        concat_attention = tf.reshape(\n            attention, (batch_size, -1, self.embed_dim)\n        )  # (batch_size, seq_len, embed_dim)\n        output = self.combine_heads(\n            concat_attention\n        )  # (batch_size, seq_len, embed_dim)\n        return output\n\n\n\"\"\"\n## Implement a Transformer block as a layer\n\"\"\"\n\n\nclass TransformerBlock(layers.Layer):\n    def __init__(self, embed_dim, num_heads, ff_dim, rate=0.1):\n        super(TransformerBlock, self).__init__()\n        self.att = MultiHeadSelfAttention(embed_dim, num_heads)\n        self.ffn = keras.Sequential(\n            [layers.Dense(ff_dim, activation=\"relu\"), layers.Dense(embed_dim),]\n        )\n        self.layernorm1 = layers.LayerNormalization(epsilon=1e-6)\n        self.layernorm2 = layers.LayerNormalization(epsilon=1e-6)\n        self.dropout1 = layers.Dropout(rate)\n        self.dropout2 = layers.Dropout(rate)\n\n    def call(self, inputs, training):\n        attn_output = self.att(inputs)\n        attn_output = self.dropout1(attn_output, training=training)\n        out1 = self.layernorm1(inputs + attn_output)\n        ffn_output = self.ffn(out1)\n        ffn_output = self.dropout2(ffn_output, training=training)\n        return self.layernorm2(out1 + ffn_output)\n\n\n\"\"\"\n## Implement embedding layer\nTwo seperate embedding layers, one for tokens, one for token index (positions).\n\"\"\"\n\n\nclass TokenAndPositionEmbedding(layers.Layer):\n    def __init__(self, maxlen, vocab_size, embed_dim):\n        super(TokenAndPositionEmbedding, self).__init__()\n        self.token_emb = layers.Embedding(input_dim=vocab_size, output_dim=embed_dim)\n        self.pos_emb = layers.Embedding(input_dim=maxlen, output_dim=embed_dim)\n\n    def call(self, x):\n        maxlen = tf.shape(x)[-1]\n        positions = tf.range(start=0, limit=maxlen, delta=1)\n        positions = self.pos_emb(positions)\n        x = self.token_emb(x)\n        return x + positions","metadata":{"execution":{"iopub.status.busy":"2022-08-31T14:56:32.909254Z","iopub.execute_input":"2022-08-31T14:56:32.909911Z","iopub.status.idle":"2022-08-31T14:56:32.930622Z","shell.execute_reply.started":"2022-08-31T14:56:32.909875Z","shell.execute_reply":"2022-08-31T14:56:32.929727Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"MAXLEN = 250\nVOCABLEN = 10000\nNCHANNEL = 3\nBATCHSIZE = 32\nEPOCHS = 60\n\nTEXTCOL = \"text\"\nTARGETCOL = \"author\"\nCHARS_TO_REMOVE = '!\"#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~\\t\\n“”’\\'∞θ÷α•à−β∅³π‘₹´°£€\\×™√²—'","metadata":{"execution":{"iopub.status.busy":"2022-08-31T14:56:41.946709Z","iopub.execute_input":"2022-08-31T14:56:41.947355Z","iopub.status.idle":"2022-08-31T14:56:41.952383Z","shell.execute_reply.started":"2022-08-31T14:56:41.947320Z","shell.execute_reply":"2022-08-31T14:56:41.951552Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"train  = pd.read_csv(\"../input/spooky-authors-csv/train.csv\")\ntest   = pd.read_csv(\"../input/spooky-authors-csv/test.csv\")\nsample = pd.read_csv(\"../input/spooky-authors-csv/sample_submission.csv\")","metadata":{"execution":{"iopub.status.busy":"2022-08-31T14:56:45.075006Z","iopub.execute_input":"2022-08-31T14:56:45.075572Z","iopub.status.idle":"2022-08-31T14:56:45.311453Z","shell.execute_reply.started":"2022-08-31T14:56:45.075528Z","shell.execute_reply":"2022-08-31T14:56:45.310466Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"xtrain = train[TEXTCOL].astype(str)\nxtest = test[TEXTCOL].astype(str)\n\ntestdex = test.id\nsub_cols = submission.columns\n\nlabel_mapper = {name: i for i,name in enumerate(set(train[TARGETCOL].values))}\nnum_label = np.vectorize(label_mapper.get)(train[TARGETCOL].values)\ny_train = to_categorical(num_label)\n\ntokenizer = text.Tokenizer(\n    filters=CHARS_TO_REMOVE,\n    lower=False,\n    num_words=VOCABLEN)\ntokenizer.fit_on_texts(list(xtrain) + list(xtest))\n\nxtrain = tokenizer.texts_to_sequences(xtrain)\nxtest = tokenizer.texts_to_sequences(xtest)\n\nxtrain = sequence.pad_sequences(xtrain, maxlen=MAXLEN)\nxtest = sequence.pad_sequences(xtest, maxlen=MAXLEN)\n\n\nprint(\"X Shape: {}\".format(xtrain.shape))\nprint(\"y Shape: {}\".format(xtrain.shape))\n\nX_train, X_val, y_train, y_val = train_test_split(\n    xtrain, y_train, test_size=0.33, random_state=42)\n\nprint(\"X_train Shape: {}\".format(X_train.shape))\nprint(\"y_train Shape: {}\".format(y_train.shape))","metadata":{"execution":{"iopub.status.busy":"2022-08-31T15:09:34.992034Z","iopub.execute_input":"2022-08-31T15:09:34.992916Z","iopub.status.idle":"2022-08-31T15:09:36.742915Z","shell.execute_reply.started":"2022-08-31T15:09:34.992869Z","shell.execute_reply":"2022-08-31T15:09:36.742020Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"code","source":"embed_dim = 32  # Embedding size for each token\nnum_heads = 4  # Number of attention heads\nff_dim = 32  # Hidden layer size in feed forward network inside transformer\n\ninputs = layers.Input(shape=(MAXLEN,))\nembedding_layer = TokenAndPositionEmbedding(MAXLEN, VOCABLEN, embed_dim)\nx = embedding_layer(inputs)\ntransformer_block = TransformerBlock(embed_dim, num_heads, ff_dim)\nx = transformer_block(x)\nx = layers.GlobalAveragePooling1D()(x)\nx = layers.Dropout(0.3)(x)\nx = layers.Dense(32, activation=\"relu\")(x)\nx = layers.Dropout(0.3)(x)\noutputs = layers.Dense(NCHANNEL, activation=\"softmax\")(x)\n\nmodel = keras.Model(inputs=inputs, outputs=outputs)","metadata":{"execution":{"iopub.status.busy":"2022-08-31T15:01:43.870972Z","iopub.execute_input":"2022-08-31T15:01:43.871616Z","iopub.status.idle":"2022-08-31T15:01:44.794185Z","shell.execute_reply.started":"2022-08-31T15:01:43.871581Z","shell.execute_reply":"2022-08-31T15:01:44.793373Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"\"\"\"\n## Train and Evaluate\n\"\"\"\n# checkpoint = callbacks.ModelCheckpoint('model.h5', monitor='val_loss', save_best_only=True)\nes = callbacks.EarlyStopping(monitor='val_loss', min_delta=0.0001,\n                             patience=4, verbose=1, mode='min', baseline=None,\n                             restore_best_weights=False)\n\n\nmodel.compile(Adam(lr=5e-5), \"categorical_crossentropy\", metrics=[\"accuracy\"])\nhistory = model.fit(\n    X_train, y_train, batch_size=BATCHSIZE, epochs=EPOCHS, validation_data=(X_val, y_val),\n    callbacks=[es]\n)","metadata":{"execution":{"iopub.status.busy":"2022-08-31T15:02:06.916916Z","iopub.execute_input":"2022-08-31T15:02:06.917381Z","iopub.status.idle":"2022-08-31T15:03:52.056281Z","shell.execute_reply.started":"2022-08-31T15:02:06.917350Z","shell.execute_reply":"2022-08-31T15:03:52.055399Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"plot_metrics = ['loss']\n\nf, ax = plt.subplots(1,figsize = [7,4])\nfor p_i,metric in enumerate(plot_metrics):\n    ax.plot(history.history[metric], label='Train ' + metric)\n    ax.plot(history.history['val_' + metric], label='Val ' + metric)\n    ax.set_title(\"Loss Curve - {}\".format(metric))\n    ax.legend()\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-08-31T15:04:38.848914Z","iopub.execute_input":"2022-08-31T15:04:38.849359Z","iopub.status.idle":"2022-08-31T15:05:30.772075Z","shell.execute_reply.started":"2022-08-31T15:04:38.849328Z","shell.execute_reply":"2022-08-31T15:05:30.771323Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"### Function to create confusion matrix ###\nimport itertools\nfrom sklearn.metrics import confusion_matrix\n\n### From http://scikit-learn.org/stable/auto_examples/model_selection/plot_confusion_matrix.html#sphx-glr-auto-examples-model-selection-plot-confusion-matrix-py #\ndef plot_confusion_matrix(cm, classes,\n                          normalize=False,\n                          title='Confusion matrix',\n                          cmap=plt.cm.Blues):\n    \"\"\"\n    This function prints and plots the confusion matrix.\n    Normalization can be applied by setting `normalize=True`.\n    \"\"\"\n    if normalize:\n        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n        #print(\"Normalized confusion matrix\")\n    #else:\n    #    print('Confusion matrix, without normalization')\n\n    #print(cm)\n\n    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n    plt.title(title)\n    plt.colorbar()\n    tick_marks = np.arange(len(classes))\n    plt.xticks(tick_marks, classes, rotation=45)\n    plt.yticks(tick_marks, classes)\n\n    fmt = '.2f' if normalize else 'd'\n    thresh = cm.max() / 2.\n    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n        plt.text(j, i, format(cm[i, j], fmt),\n                 horizontalalignment=\"center\",\n                 color=\"white\" if cm[i, j] > thresh else \"black\")\n\n    plt.tight_layout()\n    plt.ylabel('True label')\n    plt.xlabel('Predicted label')\n\n\nval_pred = model.predict(X_val)\ncnf_matrix = confusion_matrix(np.argmax(y_val,axis=1), np.argmax(val_pred,axis=1))\nnp.set_printoptions(precision=2)\n\n# Plot non-normalized confusion matrix\nplt.figure(figsize=(4,4))\nplot_confusion_matrix(cnf_matrix, classes=['EAP', 'HPL', 'MWS'],\n                      title='Confusion matrix, without normalization')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-08-31T15:06:07.111422Z","iopub.execute_input":"2022-08-31T15:06:07.111806Z","iopub.status.idle":"2022-08-31T15:06:08.343816Z","shell.execute_reply.started":"2022-08-31T15:06:07.111774Z","shell.execute_reply":"2022-08-31T15:06:08.342866Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"# model.load_weights('model.h5')\ntest_pred = model.predict(xtest)\ntest_pred.shape","metadata":{"execution":{"iopub.status.busy":"2022-08-31T15:06:35.234720Z","iopub.execute_input":"2022-08-31T15:06:35.235192Z","iopub.status.idle":"2022-08-31T15:06:36.626264Z","shell.execute_reply.started":"2022-08-31T15:06:35.235150Z","shell.execute_reply":"2022-08-31T15:06:36.625444Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"submission = pd.DataFrame(test_pred, columns=label_mapper.keys())\nsubmission['id'] = testdex\n\nsubmission = submission[sub_cols]\nsubmission.to_csv('submission_transfomer_keras.csv', index=False)\nprint(submission.shape)","metadata":{"execution":{"iopub.status.busy":"2022-08-31T15:09:46.094798Z","iopub.execute_input":"2022-08-31T15:09:46.095371Z","iopub.status.idle":"2022-08-31T15:09:46.143064Z","shell.execute_reply.started":"2022-08-31T15:09:46.095312Z","shell.execute_reply":"2022-08-31T15:09:46.142173Z"},"trusted":true},"execution_count":17,"outputs":[]},{"cell_type":"code","source":"!head submission_transfomer_keras.csv","metadata":{"execution":{"iopub.status.busy":"2022-08-31T15:09:48.576719Z","iopub.execute_input":"2022-08-31T15:09:48.577567Z","iopub.status.idle":"2022-08-31T15:09:49.604908Z","shell.execute_reply.started":"2022-08-31T15:09:48.577522Z","shell.execute_reply":"2022-08-31T15:09:49.603701Z"},"trusted":true},"execution_count":18,"outputs":[]},{"cell_type":"code","source":"print(\"Notebook Runtime: %0.2f Minutes\"%((time.time() - notebookstart)/60))","metadata":{"execution":{"iopub.status.busy":"2022-08-31T15:09:56.563529Z","iopub.execute_input":"2022-08-31T15:09:56.564177Z","iopub.status.idle":"2022-08-31T15:09:56.569835Z","shell.execute_reply.started":"2022-08-31T15:09:56.564132Z","shell.execute_reply":"2022-08-31T15:09:56.568973Z"},"trusted":true},"execution_count":19,"outputs":[]}]}